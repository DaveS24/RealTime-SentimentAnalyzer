{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe from pickle\n",
    "sentiment_data_df = pd.read_pickle('./data/sentiment_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "sentiment_data_df['no_punctuation_text'] = sentiment_data_df['tokenized_text'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "# Remove stopwords\n",
    "custom_stopwords = set(stopwords.words('english')) - {'but', 'not', 'no', 'nor'}\n",
    "sentiment_data_df['no_stopwords_text'] = sentiment_data_df['tokenized_text'].apply(lambda x: [word for word in x if word not in custom_stopwords])\n",
    "\n",
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "sentiment_data_df['stemmed_text'] = sentiment_data_df['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentiment_data_df['lemmatized_text'] = sentiment_data_df['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase ids</th>\n",
       "      <th>sentiment values</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>no_punctuation_text</th>\n",
       "      <th>no_stopwords_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>226166.0</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, rock, is, destined, to, be, the, 21st, c...</td>\n",
       "      <td>[the, rock, is, destined, to, be, the, 21st, c...</td>\n",
       "      <td>[rock, destined, 21st, century, 's, new, ``, c...</td>\n",
       "      <td>[the, rock, is, destin, to, be, the, 21st, cen...</td>\n",
       "      <td>[the, rock, is, destined, to, be, the, 21st, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>226300.0</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, gorgeously, elaborate, continuation, of,...</td>\n",
       "      <td>[the, gorgeously, elaborate, continuation, of,...</td>\n",
       "      <td>[gorgeously, elaborate, continuation, ``, lord...</td>\n",
       "      <td>[the, gorgeous, elabor, continu, of, ``, the, ...</td>\n",
       "      <td>[the, gorgeously, elaborate, continuation, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>13995.0</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>2</td>\n",
       "      <td>[effective, but, too-tepid, biopic]</td>\n",
       "      <td>[effective, but, too-tepid, biopic]</td>\n",
       "      <td>[effective, but, too-tepid, biopic]</td>\n",
       "      <td>[effect, but, too-tepid, biopic]</td>\n",
       "      <td>[effective, but, too-tepid, biopic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>14123.0</td>\n",
       "      <td>0.73611</td>\n",
       "      <td>2</td>\n",
       "      <td>[if, you, sometimes, like, to, go, to, the, mo...</td>\n",
       "      <td>[if, you, sometimes, like, to, go, to, the, mo...</td>\n",
       "      <td>[sometimes, like, go, movies, fun, ,, wasabi, ...</td>\n",
       "      <td>[if, you, sometim, like, to, go, to, the, movi...</td>\n",
       "      <td>[if, you, sometimes, like, to, go, to, the, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>13999.0</td>\n",
       "      <td>0.86111</td>\n",
       "      <td>2</td>\n",
       "      <td>[emerges, as, something, rare, ,, an, issue, m...</td>\n",
       "      <td>[emerges, as, something, rare, an, issue, movi...</td>\n",
       "      <td>[emerges, something, rare, ,, issue, movie, 's...</td>\n",
       "      <td>[emerg, as, someth, rare, ,, an, issu, movi, t...</td>\n",
       "      <td>[emerges, a, something, rare, ,, an, issue, mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence  \\\n",
       "0               1  The Rock is destined to be the 21st Century 's...   \n",
       "1               2  The gorgeously elaborate continuation of `` Th...   \n",
       "2               3                     Effective but too-tepid biopic   \n",
       "3               4  If you sometimes like to go to the movies to h...   \n",
       "4               5  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "                                              phrase  phrase ids  \\\n",
       "0  The Rock is destined to be the 21st Century 's...    226166.0   \n",
       "1  The gorgeously elaborate continuation of `` Th...    226300.0   \n",
       "2                     Effective but too-tepid biopic     13995.0   \n",
       "3  If you sometimes like to go to the movies to h...     14123.0   \n",
       "4  Emerges as something rare , an issue movie tha...     13999.0   \n",
       "\n",
       "   sentiment values  splitset_label  \\\n",
       "0           0.69444               1   \n",
       "1           0.83333               1   \n",
       "2           0.51389               2   \n",
       "3           0.73611               2   \n",
       "4           0.86111               2   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [the, rock, is, destined, to, be, the, 21st, c...   \n",
       "1  [the, gorgeously, elaborate, continuation, of,...   \n",
       "2                [effective, but, too-tepid, biopic]   \n",
       "3  [if, you, sometimes, like, to, go, to, the, mo...   \n",
       "4  [emerges, as, something, rare, ,, an, issue, m...   \n",
       "\n",
       "                                 no_punctuation_text  \\\n",
       "0  [the, rock, is, destined, to, be, the, 21st, c...   \n",
       "1  [the, gorgeously, elaborate, continuation, of,...   \n",
       "2                [effective, but, too-tepid, biopic]   \n",
       "3  [if, you, sometimes, like, to, go, to, the, mo...   \n",
       "4  [emerges, as, something, rare, an, issue, movi...   \n",
       "\n",
       "                                   no_stopwords_text  \\\n",
       "0  [rock, destined, 21st, century, 's, new, ``, c...   \n",
       "1  [gorgeously, elaborate, continuation, ``, lord...   \n",
       "2                [effective, but, too-tepid, biopic]   \n",
       "3  [sometimes, like, go, movies, fun, ,, wasabi, ...   \n",
       "4  [emerges, something, rare, ,, issue, movie, 's...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  [the, rock, is, destin, to, be, the, 21st, cen...   \n",
       "1  [the, gorgeous, elabor, continu, of, ``, the, ...   \n",
       "2                   [effect, but, too-tepid, biopic]   \n",
       "3  [if, you, sometim, like, to, go, to, the, movi...   \n",
       "4  [emerg, as, someth, rare, ,, an, issu, movi, t...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [the, rock, is, destined, to, be, the, 21st, c...  \n",
       "1  [the, gorgeously, elaborate, continuation, of,...  \n",
       "2                [effective, but, too-tepid, biopic]  \n",
       "3  [if, you, sometimes, like, to, go, to, the, mo...  \n",
       "4  [emerges, a, something, rare, ,, an, issue, mo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11286 entries, 0 to 11854\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   sentence_index       11286 non-null  int64  \n",
      " 1   sentence             11286 non-null  object \n",
      " 2   phrase               11286 non-null  object \n",
      " 3   phrase ids           11286 non-null  float64\n",
      " 4   sentiment values     11286 non-null  float64\n",
      " 5   splitset_label       11286 non-null  int64  \n",
      " 6   tokenized_text       11286 non-null  object \n",
      " 7   no_punctuation_text  11286 non-null  object \n",
      " 8   no_stopwords_text    11286 non-null  object \n",
      " 9   stemmed_text         11286 non-null  object \n",
      " 10  lemmatized_text      11286 non-null  object \n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(sentiment_data_df['tokenized_text'].apply(lambda x: ' '.join(x)))\n",
    "sequences = tokenizer.texts_to_sequences(sentiment_data_df['tokenized_text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "sentiment_data = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (8117, 100)\n",
      "Test data shape:  (2125, 100)\n",
      "Dev data shape:  (1044, 100)\n"
     ]
    }
   ],
   "source": [
    "train_data = sentiment_data[sentiment_data_df['splitset_label'] == 1]\n",
    "test_data = sentiment_data[sentiment_data_df['splitset_label'] == 2]\n",
    "dev_data = sentiment_data[sentiment_data_df['splitset_label'] == 3]\n",
    "\n",
    "print('Train data shape: ', train_data.shape)\n",
    "print('Test data shape: ', test_data.shape)\n",
    "print('Dev data shape: ', dev_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
